{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc4bcb8",
   "metadata": {},
   "source": [
    "# Finetuning Pre-trained Models — From Theory to Practice\n",
    "\n",
    "**Framework:** PyTorch + Hugging Face Transformers  \n",
    "**Models:** Light models (CPU/Colab friendly)  \n",
    "**Datasets:** Small public samples (IMDB, AG News, CoNLL2003)  \n",
    "**Covers:** Full finetuning, Feature Extraction, Adapters, Prefix/Prompt Tuning, LoRA, NER, Vision TL, Eval, Deployment\n",
    "\n",
    "> Tip: Run this on Google Colab with a T4/A100 GPU for the LoRA and generation examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce1f5b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Setup\n",
    "2. Concepts & Heuristics\n",
    "3. Data Loading & Preparation\n",
    "4. Full Finetuning (Text Classification)\n",
    "5. Feature Extraction (Frozen Encoder + Head)\n",
    "6. Adapters (Parameter-Efficient)\n",
    "7. Prefix/Prompt Tuning (Generation)\n",
    "8. LoRA (Parameter-Efficient Finetuning)\n",
    "9. Token Classification (NER)\n",
    "10. Vision Transfer Learning (Brief)\n",
    "11. Evaluation & Monitoring\n",
    "12. Deployment (Pipeline + FastAPI)\n",
    "13. Cost/Quality Heuristics\n",
    "14. Pitfalls & Debugging\n",
    "15. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98308798",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "This installs required libraries. Skip installs if your environment already has them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5579db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers datasets accelerate evaluate scikit-learn seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional (for Adapters & PEFT/LoRA; may require GPU for speed)\n",
    "!pip -q install peft adapter-transformers bitsandbytes==0.43.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d190539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, os, math, random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, AutoModel,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    AutoModelForTokenClassification, DataCollatorForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "print('Torch:', torch.__version__)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b40a2",
   "metadata": {},
   "source": [
    "## 2. Concepts & Heuristics\n",
    "\n",
    "**Finetuning** = continue training a pre-trained model on task-/domain-specific data.  \n",
    "**When to choose what:**  \n",
    "- **Prompting/RAG**: tiny data, zero engineering budget.  \n",
    "- **Feature Extraction**: quick baseline; freeze encoder, train head.  \n",
    "- **Adapters / Prefix**: low compute, many domains.  \n",
    "- **LoRA/QLoRA**: best quality/cost for LLMs; train small low-rank matrices.  \n",
    "- **Full Finetune**: small models (≤400M) with enough data/GPU; precise control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9944cf",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Preparation\n",
    "\n",
    "We'll use small, public datasets to keep the notebook lightweight:\n",
    "- **AG News** (4-class text classification)\n",
    "- **IMDB** (binary sentiment) — optional\n",
    "- **CoNLL2003** (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News (classification)\n",
    "from datasets import load_dataset, DatasetDict\n",
    "ag = load_dataset(\"ag_news\")\n",
    "# Subsample for speed\n",
    "ag_small = DatasetDict({\n",
    "    'train': ag['train'].shuffle(seed=42).select(range(2000)),\n",
    "    'validation': ag['test'].shuffle(seed=42).select(range(1000))\n",
    "})\n",
    "ag_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a1815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CoNLL2003 (NER)\n",
    "conll = load_dataset(\"conll2003\")\n",
    "conll_small = DatasetDict({\n",
    "    'train': conll['train'].shuffle(seed=42).select(range(2000)),\n",
    "    'validation': conll['validation'].shuffle(seed=42).select(range(1000))\n",
    "})\n",
    "label_list = conll['train'].features['ner_tags'].feature.names\n",
    "id2label = {i:l for i,l in enumerate(label_list)}\n",
    "label2id = {l:i for i,l in enumerate(label_list)}\n",
    "label_list[:10], len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993416b",
   "metadata": {},
   "source": [
    "## 4. Full Finetuning (Text Classification)\n",
    "\n",
    "**Definition:** Update **all** parameters of a pre-trained encoder for the downstream task.  \n",
    "**When:** Small/medium models, enough data, need best accuracy.\n",
    "\n",
    "We'll use **DistilBERT** on **AG News**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ddd393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tok_ag(batch):\n",
    "    return tok(batch['text'], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "ag_tok = ag_small.map(tok_ag, batched=True)\n",
    "ag_tok = ag_tok.rename_column('label', 'labels')\n",
    "ag_tok.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "\n",
    "model_full = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "dc = DataCollatorWithPadding(tok)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average='macro')\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ft_full_agnews\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_full, args=args,\n",
    "    train_dataset=ag_tok['train'],\n",
    "    eval_dataset=ag_tok['validation'],\n",
    "    tokenizer=tok,\n",
    "    data_collator=dc,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ef173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and preview predictions\n",
    "preds = trainer.predict(ag_tok['validation'])\n",
    "print(preds.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619bbd4e",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction (Frozen Encoder + Head)\n",
    "\n",
    "**Definition:** Freeze the encoder and train a small classification head.  \n",
    "**Why:** Very fast, good baseline, reduces overfitting on small data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f53e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "encoder = AutoModel.from_pretrained(model_name)\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "class ClsHead(nn.Module):\n",
    "    def __init__(self, hidden, n_labels):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(hidden, n_labels)\n",
    "    def forward(self, last_hidden_state, attention_mask=None):\n",
    "        pooled = last_hidden_state[:,0]  # [CLS]-like pooling\n",
    "        return self.fc(self.dropout(pooled))\n",
    "\n",
    "head = ClsHead(hidden=encoder.config.hidden_size, n_labels=4)\n",
    "\n",
    "# Skeleton training loop over tokenized AG News features\n",
    "optim = torch.optim.AdamW(head.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def batch_iter(ds, bs=32):\n",
    "    n = len(ds[\"input_ids\"])\n",
    "    for i in range(0, n, bs):\n",
    "        yield {k: v[i:i+bs] for k,v in ds.items() if hasattr(v, \"shape\")}\n",
    "\n",
    "train_set = ag_tok['train']\n",
    "for step, batch in enumerate(batch_iter(train_set, bs=32)):\n",
    "    with torch.no_grad():\n",
    "        out = encoder(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "    logits = head(out.last_hidden_state)\n",
    "    loss = loss_fn(logits, batch['labels'])\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % 50 == 0:\n",
    "        print('step', step, 'loss', float(loss))\n",
    "    if step > 200: break  # keep light"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789fcc7",
   "metadata": {},
   "source": [
    "## 6. Adapters (Parameter-Efficient)\n",
    "\n",
    "**Definition:** Insert small trainable bottlenecks inside each Transformer block; freeze the base.  \n",
    "**Benefits:** Swap adapters for domains; small memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca69c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import transformers.adapters.composition as ac\n",
    "\n",
    "model_ad = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "model_ad.add_adapter(\"agnews_adapter\", config=\"pfeiffer\")\n",
    "model_ad.train_adapter(\"agnews_adapter\")\n",
    "\n",
    "trainer_ad = Trainer(\n",
    "    model=model_ad, args=TrainingArguments(\n",
    "        output_dir=\"ft_adapters_agnews\",\n",
    "        per_device_train_batch_size=16, per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1, evaluation_strategy=\"epoch\", learning_rate=5e-4,\n",
    "        logging_steps=50\n",
    "    ),\n",
    "    train_dataset=ag_tok['train'], eval_dataset=ag_tok['validation'],\n",
    "    tokenizer=tok, data_collator=dc, compute_metrics=compute_metrics\n",
    ")\n",
    "trainer_ad.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a4cca",
   "metadata": {},
   "source": [
    "## 7. Prefix/Prompt Tuning (Generation)\n",
    "\n",
    "**Definition:** Learn soft prompt vectors prepended to inputs; keep base frozen.  \n",
    "We'll demonstrate with a **small causal LM** (`gpt2`). For better results, use a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a76477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PromptTuningConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_causal = \"gpt2\"\n",
    "tok_g = AutoTokenizer.from_pretrained(base_causal)\n",
    "tok_g.pad_token = tok_g.eos_token\n",
    "\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(base_causal)\n",
    "\n",
    "pt_cfg = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=20)\n",
    "gpt2_pt = get_peft_model(gpt2, pt_cfg)\n",
    "gpt2_pt.print_trainable_parameters()\n",
    "\n",
    "prompt = \"Write a one-sentence product description for a fintech savings app:\"\n",
    "ids = tok_g(prompt, return_tensors='pt')\n",
    "out = gpt2_pt.generate(**ids, max_new_tokens=40)\n",
    "print(tok_g.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174fa4c",
   "metadata": {},
   "source": [
    "## 8. LoRA (PEFT) — Lightweight Demo\n",
    "\n",
    "**LoRA:** Train low-rank matrices on top of frozen weights; great quality/cost trade-off.  \n",
    "We'll show setup on a small causal LM (e.g., `gpt2`) for demonstration. For serious finetunes, use larger models + GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc300f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "lora_cfg = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "                      task_type=\"CAUSAL_LM\", target_modules=[\"c_attn\",\"c_proj\"])\n",
    "gpt2_lora = get_peft_model(gpt2, lora_cfg)\n",
    "gpt2_lora.print_trainable_parameters()\n",
    "\n",
    "# Tiny toy dataset for SFT-style tuning (demo only)\n",
    "train_texts = [\n",
    "    \"Instruction: Summarize the benefit of automatic savings.\\nAnswer: It helps users build funds effortlessly over time.\",\n",
    "    \"Instruction: Explain KYC in one line.\\nAnswer: KYC is the identity verification required by financial institutions.\",\n",
    "    \"Instruction: Write a friendly reminder about overdraft fees.\\nAnswer: Remember to maintain your balance to avoid overdraft fees.\"\n",
    "]\n",
    "toy = [{\"text\": t} for t in train_texts]\n",
    "\n",
    "def tok_txt(row): \n",
    "    return tok_g(row[\"text\"], truncation=True, max_length=128)\n",
    "toy_ds = Dataset.from_list(toy).map(lambda x: tok_txt(x))\n",
    "toy_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\"])\n",
    "\n",
    "dc_lm = DataCollatorForLanguageModeling(tok_g, mlm=False)\n",
    "args_lora = TrainingArguments(\n",
    "    output_dir=\"gpt2_lora_toy\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=5\n",
    ")\n",
    "trainer_lora = Trainer(model=gpt2_lora, args=args_lora, train_dataset=toy_ds, data_collator=dc_lm)\n",
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try generating again (adapter still attached)\n",
    "prompt = \"Instruction: Write a friendly one-line savings tip.\\nAnswer:\"\n",
    "ids = tok_g(prompt, return_tensors='pt')\n",
    "out = gpt2_lora.generate(**ids, max_new_tokens=40)\n",
    "print(tok_g.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ec6f6",
   "metadata": {},
   "source": [
    "## 9. Token Classification (NER)\n",
    "\n",
    "**Task:** Assign an entity label to each token (e.g., PER, ORG, LOC).  \n",
    "We'll finetune a small BERT on a **subset of CoNLL2003**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification, AutoTokenizer\n",
    "tok_n = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized = tok_n(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        ids = tokenized.word_ids(batch_index=i)\n",
    "        ex_labels = examples[\"ner_tags\"][i]\n",
    "        aligned = []\n",
    "        prev = None\n",
    "        for wid in ids:\n",
    "            if wid is None:\n",
    "                aligned.append(-100)\n",
    "            elif wid != prev:\n",
    "                aligned.append(ex_labels[wid])\n",
    "            else:\n",
    "                aligned.append(-100)\n",
    "            prev = wid\n",
    "        labels.append(aligned)\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "conll_tok = conll_small.map(tokenize_and_align_labels, batched=True)\n",
    "cols = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "conll_tok.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\", num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
    ")\n",
    "dc_ner = DataCollatorForTokenClassification(tok_n)\n",
    "\n",
    "args_ner = TrainingArguments(\n",
    "    output_dir=\"ner_bert_conll\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=50\n",
    ")\n",
    "trainer_ner = Trainer(\n",
    "    model=ner_model, args=args_ner,\n",
    "    train_dataset=conll_tok['train'], eval_dataset=conll_tok['validation'],\n",
    "    data_collator=dc_ner, tokenizer=tok_n\n",
    ")\n",
    "trainer_ner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e08311",
   "metadata": {},
   "source": [
    "## 10. Vision Transfer Learning (Brief)\n",
    "\n",
    "**Idea:** Start from an ImageNet pre-trained model (e.g., ResNet18), freeze most layers, train a small classifier head for your classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb992cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 5)  # example 5 classes\n",
    "sum(p.numel() for p in resnet.parameters() if p.requires_grad), 'trainable params in head'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6568d69",
   "metadata": {},
   "source": [
    "## 11. Evaluation & Monitoring\n",
    "\n",
    "- **Classification:** accuracy, macro-F1, confusion matrix  \n",
    "- **Generation:** ROUGE/BLEU, human eval, safety filters  \n",
    "- **NER:** precision/recall/F1 (seqeval)  \n",
    "- **Monitoring:** drift detection, periodic re-eval, canary prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "# Example: ROUGE for generation\n",
    "rouge = load(\"rouge\")\n",
    "preds = [\"The cat sat on the mat.\"]\n",
    "refs = [\"A cat is sitting on a mat.\"]\n",
    "rouge.compute(predictions=preds, references=refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de2e8f1",
   "metadata": {},
   "source": [
    "## 12. Deployment\n",
    "\n",
    "### Option A — Simple Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline(\"text-classification\", model=model_full, tokenizer=tok, device=-1)\n",
    "clf(\"Breaking: New economic policy announced by central bank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a158760",
   "metadata": {},
   "source": [
    "### Option B — FastAPI Endpoint (skeleton)\n",
    "> Save as `serve.py` and run with `uvicorn serve:app --reload`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Inp(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(inp: Inp):\n",
    "    out = clf(inp.text)[0]\n",
    "    return {\"label\": out['label'], \"score\": float(out['score'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e8fc1",
   "metadata": {},
   "source": [
    "## 13. Cost/Quality Heuristics\n",
    "\n",
    "- **Low compute:** Adapters / Prefix / LoRA  \n",
    "- **Many domains:** One base + per-domain adapters  \n",
    "- **Tiny dataset:** Feature extraction baseline, then PEFT  \n",
    "- **Latency critical:** Smaller base + quantization; distill later  \n",
    "- **Regulated domains:** Instruction SFT + guardrails + audits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8acb6",
   "metadata": {},
   "source": [
    "## 14. Pitfalls & Debugging\n",
    "\n",
    "| Issue | Symptom | Fix |\n",
    "|---|---|---|\n",
    "| Overfitting | train↑, val↓ | more data, dropout, early stop |\n",
    "| Catastrophic forgetting | generic ability↓ | reduce LR, PEFT, data mix |\n",
    "| Tokenization mismatch | errors, poor perf | use original tokenizer |\n",
    "| Label leakage | unrealistically high val | fix splits & pipelines |\n",
    "| Hallucination (LLMs) | incorrect facts | RAG, stricter prompts, eval |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8bf1ff",
   "metadata": {},
   "source": [
    "## 15. Exercises\n",
    "\n",
    "1. Replace DistilBERT with `bert-base-uncased` and re-run section 4. Compare F1.\n",
    "2. Change max sequence length to 128 vs 256. Observe speed/accuracy trade-off.\n",
    "3. Add a confusion matrix for AG News validation.\n",
    "4. Train an additional adapter named `finance_v2` and compare adapter-only metrics.\n",
    "5. For LoRA, increase rank `r` from 8 → 16 and measure generation changes.\n",
    "6. Build a tiny NER dataset from your domain (5–10 examples) and test few-shot finetune.\n",
    "7. (Vision) Unfreeze the last *two* ResNet layers and fine-tune at a lower LR."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
