{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2041636",
   "metadata": {},
   "source": [
    "# Model Evaluation and Testing — From Development to Production\n",
    "\n",
    "**Audience:** ML developers & data scientists  \n",
    "**Scope:** Metrics, testing methodology, offline/online evaluation, monitoring & drift  \n",
    "**Includes:** Explanations, formulas (LaTeX), and runnable Python code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84ea23",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Setup\n",
    "2. Datasets & Reproducibility\n",
    "3. Data Quality & Split Validation\n",
    "4. Classification Metrics (formulas + code)\n",
    "5. Regression Metrics (formulas + code)\n",
    "6. Calibration & Thresholding\n",
    "7. Curves & Diagnostic Plots (ROC/PR, Confusion Matrix)\n",
    "8. Clustering Metrics (formulas + code)\n",
    "9. Cross-Validation & Hyperparameter Tuning\n",
    "10. Robustness, Bias & Fairness (practical checks)\n",
    "11. Performance, Latency & Throughput (quick benchmarks)\n",
    "12. Drift & Monitoring (with formulas + code sketch)\n",
    "13. Testing in MLOps (unit, integration, regression tests)\n",
    "14. Developer Checklists & Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215ebb4",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install scikit-learn matplotlib numpy pandas evaluate shap fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, os, math, time, sys, itertools\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, roc_curve, precision_recall_curve,\n",
    "    average_precision_score, mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs, load_diabetes, load_breast_cancer\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191e993",
   "metadata": {},
   "source": [
    "## 2. Datasets & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "X_cls, y_cls = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, stratify=y_cls, random_state=42\n",
    ")\n",
    "\n",
    "# Regression\n",
    "X_reg, y_reg = load_diabetes(return_X_y=True, as_frame=True)\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Clustering\n",
    "from sklearn.datasets import make_blobs\n",
    "X_clu, y_clu_true = make_blobs(n_samples=600, centers=4, cluster_std=1.2, random_state=42)\n",
    "\n",
    "(X_cls.head(), X_reg.head(), X_clu[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4dc26",
   "metadata": {},
   "source": [
    "## 3. Data Quality & Split Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19016d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_profile(df: pd.DataFrame, name: str):\n",
    "    print(f\"--- {name} shape:\", df.shape)\n",
    "    print(\"Missing values per column:\\n\", df.isna().sum().sort_values(ascending=False)[:5])\n",
    "    print(\"\\nDescribe:\\n\", df.describe().T.head())\n",
    "\n",
    "quick_profile(Xc_train, \"Xc_train\")\n",
    "quick_profile(Xc_test, \"Xc_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def ks_split_check(train_df, test_df, top_n=5):\n",
    "    stats = []\n",
    "    for col in train_df.columns:\n",
    "        stat, p = ks_2samp(train_df[col].values, test_df[col].values)\n",
    "        stats.append((col, stat, p))\n",
    "    stats = sorted(stats, key=lambda x: -x[1])[:top_n]\n",
    "    print(\"Top features by KS statistic (train vs test):\")\n",
    "    for col, stat, p in stats:\n",
    "        print(f\"{col:25s} KS={stat:.3f}  p={p:.3f}\")\n",
    "\n",
    "ks_split_check(Xc_train, Xc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1dfe6",
   "metadata": {},
   "source": [
    "## 4. Classification Metrics — Formulas & Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaedff1",
   "metadata": {},
   "source": [
    "Let $TP$, $TN$, $FP$, $FN$ denote confusion matrix components.\n",
    "\n",
    "- **Accuracy:** $\\\\frac{TP + TN}{TP + TN + FP + FN}$  \n",
    "- **Precision:** $\\\\frac{TP}{TP + FP}$  \n",
    "- **Recall:** $\\\\frac{TP}{TP + FN}$  \n",
    "- **F1:** $2\\\\cdot\\\\frac{PR}{P+R}$ with $P$ = Precision, $R$ = Recall  \n",
    "- **ROC AUC:** area under ROC; $TPR=\\\\frac{TP}{TP+FN}$, $FPR=\\\\frac{FP}{FP+TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e770e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cls = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500, solver=\"lbfgs\"))\n",
    "pipe_cls.fit(Xc_train, yc_train)\n",
    "\n",
    "y_pred = pipe_cls.predict(Xc_test)\n",
    "y_proba = pipe_cls.predict_proba(Xc_test)[:,1]\n",
    "\n",
    "acc = accuracy_score(yc_test, y_pred)\n",
    "prec = precision_score(yc_test, y_pred)\n",
    "rec = recall_score(yc_test, y_pred)\n",
    "f1 = f1_score(yc_test, y_pred)\n",
    "auc = roc_auc_score(yc_test, y_proba)\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f} | ROC-AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a4dfd",
   "metadata": {},
   "source": [
    "## 5. Regression Metrics — Formulas & Code\n",
    "\n",
    "- **MAE:** $\\\\frac{1}{n}\\\\sum |y-\\\\hat{y}|$  \n",
    "- **MSE:** $\\\\frac{1}{n}\\\\sum (y-\\\\hat{y})^2$  \n",
    "- **RMSE:** $\\\\sqrt{\\\\mathrm{MSE}}$  \n",
    "- **$R^2$:** $1-\\\\frac{\\\\sum (y-\\\\hat{y})^2}{\\\\sum (y-\\\\bar{y})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b169c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_reg = make_pipeline(StandardScaler(), RandomForestRegressor(n_estimators=200, random_state=42))\n",
    "pipe_reg.fit(Xr_train, yr_train)\n",
    "yhat = pipe_reg.predict(Xr_test)\n",
    "\n",
    "import math\n",
    "mae = mean_absolute_error(yr_test, yhat)\n",
    "mse = mean_squared_error(yr_test, yhat)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(yr_test, yhat)\n",
    "\n",
    "print(f\"MAE: {mae:.3f} | MSE: {mse:.3f} | RMSE: {rmse:.3f} | R^2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa25426",
   "metadata": {},
   "source": [
    "## 6. Calibration & Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(yc_test, y_proba, n_bins=10, strategy=\"uniform\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot([0,1],[0,1], linestyle='--')\n",
    "plt.plot(prob_pred, prob_true, marker='o')\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Observed frequency\")\n",
    "plt.title(\"Reliability Diagram\")\n",
    "plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e44cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold tuning for best F1\n",
    "ths = np.linspace(0, 1, 101)\n",
    "f1s = []\n",
    "for t in ths:\n",
    "    y_hat = (y_proba >= t).astype(int)\n",
    "    f1s.append(f1_score(yc_test, y_hat))\n",
    "\n",
    "best_t = ths[int(np.argmax(f1s))]\n",
    "print(\"Best threshold by F1:\", best_t, \"F1:\", max(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb87500",
   "metadata": {},
   "source": [
    "## 7. Curves & Diagnostic Plots (ROC/PR, Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score, ConfusionMatrixDisplay\n",
    "\n",
    "fpr, tpr, _ = roc_curve(yc_test, y_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1],[0,1], linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\"); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee385ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall\n",
    "precisions, recalls, _ = precision_recall_curve(yc_test, y_proba)\n",
    "ap = average_precision_score(yc_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.step(recalls, precisions, where='post')\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Curve (AP={ap:.3f})\")\n",
    "plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6deb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(yc_test, y_pred, labels=[0,1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\",\"Positive\"])\n",
    "disp.plot(values_format='d'); plt.title(\"Confusion Matrix\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d539b1c",
   "metadata": {},
   "source": [
    "## 8. Clustering Metrics — Formulas & Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ecefe",
   "metadata": {},
   "source": [
    "Silhouette for sample $i$: $s(i)=\\\\frac{b(i)-a(i)}{\\\\max(a(i),b(i))}$,  \n",
    "where $a(i)$ = mean intra-cluster distance, $b(i)$ = mean distance to nearest other cluster.\n",
    "\n",
    "DBI (lower better) and CHI (higher better) provide global clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_clu)\n",
    "\n",
    "sil = silhouette_score(X_clu, labels)\n",
    "dbi = davies_bouldin_score(X_clu, labels)\n",
    "chi = calinski_harabasz_score(X_clu, labels)\n",
    "\n",
    "print(f\"Silhouette: {sil:.3f} | DBI (lower better): {dbi:.3f} | CHI (higher better): {chi:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b642f8",
   "metadata": {},
   "source": [
    "## 9. Cross-Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500))\n",
    "scores = cross_val_score(clf, X_cls, y_cls, cv=cv, scoring=\"f1\")\n",
    "print(\"5-fold CV F1:\", scores, \"Mean:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"logisticregression__C\":[0.01, 0.1, 1, 10]}\n",
    "gs = GridSearchCV(\n",
    "    make_pipeline(StandardScaler(), LogisticRegression(max_iter=500)),\n",
    "    param_grid=param_grid, scoring=\"f1\", cv=cv, n_jobs=-1\n",
    ")\n",
    "gs.fit(Xc_train, yc_train)\n",
    "print(\"Best params:\", gs.best_params_, \"Best CV F1:\", gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31f0f7",
   "metadata": {},
   "source": [
    "## 10. Robustness, Bias & Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d341a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple subgroup performance demo using a synthetic group\n",
    "group = (Xc_test.iloc[:,0] > Xc_test.iloc[:,0].median()).astype(int).values\n",
    "\n",
    "def subgroup_metrics(y_true, y_prob, g):\n",
    "    out = {}\n",
    "    for val in [0,1]:\n",
    "        idx = (g==val)\n",
    "        yhat = (y_prob[idx] >= 0.5).astype(int)\n",
    "        out[f\"group{val}_f1\"] = f1_score(y_true[idx], yhat)\n",
    "        out[f\"group{val}_recall\"] = recall_score(y_true[idx], yhat)\n",
    "        out[f\"group{val}_precision\"] = precision_score(y_true[idx], yhat)\n",
    "    return out\n",
    "\n",
    "subgroup_metrics(yc_test.values, y_proba, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate, false_positive_rate\n",
    "\n",
    "mf = MetricFrame(\n",
    "    metrics={\"selection_rate\": selection_rate, \"tpr\": true_positive_rate, \"fpr\": false_positive_rate},\n",
    "    y_true=yc_test, y_pred=(y_proba>=0.5).astype(int), sensitive_features=group\n",
    ")\n",
    "print(\"By-group metrics:\\n\", mf.by_group)\n",
    "print(\"\\nOverall disparity (range):\")\n",
    "for name, s in mf.by_group.items():\n",
    "    print(name, \"range:\", s.max() - s.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87749692",
   "metadata": {},
   "source": [
    "## 11. Performance, Latency & Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5406cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = Xc_test.values[:1024]\n",
    "start = time.time()\n",
    "_ = pipe_cls.predict_proba(X_batch)\n",
    "elapsed = time.time() - start\n",
    "latency_ms = (elapsed / len(X_batch)) * 1000\n",
    "tps = len(X_batch) / elapsed\n",
    "print(f\"Avg latency: {latency_ms:.3f} ms/sample | Throughput: {tps:.1f} samples/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725a9db",
   "metadata": {},
   "source": [
    "## 12. Drift & Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b614c60a",
   "metadata": {},
   "source": [
    "Population Stability Index (PSI): $\\\\sum_i (o_i-e_i)\\\\ln\\\\left(\\\\frac{o_i}{e_i}\\\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(expected, observed, bins=10):\n",
    "    e_perc, _ = np.histogram(expected, bins=bins, range=(np.min(expected), np.max(expected)), density=True)\n",
    "    o_perc, _ = np.histogram(observed, bins=bins, range=(np.min(expected), np.max(expected)), density=True)\n",
    "    e_perc = np.where(e_perc==0, 1e-6, e_perc)\n",
    "    o_perc = np.where(o_perc==0, 1e-6, o_perc)\n",
    "    return np.sum((o_perc - e_perc) * np.log(o_perc / e_perc))\n",
    "\n",
    "prod_feature = Xc_test.iloc[:,0].values + np.random.normal(0, 0.2, size=len(Xc_test))\n",
    "psi_val = psi(Xc_train.iloc[:,0].values, prod_feature, bins=10)\n",
    "print(\"PSI (feature 0):\", psi_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac99e8b",
   "metadata": {},
   "source": [
    "## 13. Testing in MLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_not_regressed(old_f1: float, new_f1: float, drop_tol: float = 0.01):\n",
    "    if new_f1 + drop_tol < old_f1:\n",
    "        raise AssertionError(f\"Model regressed: old_f1={old_f1:.3f}, new_f1={new_f1:.3f} exceeds tolerance {drop_tol}\")\n",
    "    return True\n",
    "\n",
    "baseline_f1 = 0.95\n",
    "current_f1 = f1\n",
    "assert_not_regressed(baseline_f1, current_f1, drop_tol=0.05)\n",
    "print(\"Regression test passed within tolerance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900914a9",
   "metadata": {},
   "source": [
    "## 14. Developer Checklists & Tips\n",
    "\n",
    "- Align metrics to business costs (precision/recall trade-offs).  \n",
    "- Stratified splits, leakage checks, and data validation.  \n",
    "- Track metrics & params (MLflow/DVC).  \n",
    "- Add CI checks: unit, integration, model regression.  \n",
    "- Monitor drift (PSI/KS) & alerting; recalibrate thresholds after prevalence shifts.  \n",
    "- Use SHAP/LIME for transparency where required."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
