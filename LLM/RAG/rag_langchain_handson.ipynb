{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c829c0a",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) — Hands‑On with LangChain\n",
    "\n",
    "**Goal:** Learn RAG from first principles and build an end‑to‑end RAG app with LangChain.  \n",
    "We will cover: concepts, components, data ingestion, chunking, embeddings, vector stores, retrievers, generation, evaluation basics, and advanced RAG patterns.  \n",
    "We’ll use **open‑source tooling** wherever possible (FAISS, HuggingFace embeddings, and Ollama for local LLMs such as `mistral`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f9bff",
   "metadata": {},
   "source": [
    "## How to use this notebook\n",
    "\n",
    "- Execute cells **top‑to‑bottom**.  \n",
    "- If you don’t have the packages, run the optional `pip install` cell.  \n",
    "- If you don’t run Ollama locally, swap the LLM to any provider you have (e.g., OpenAI, Anthropic) — code comments show how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: install dependencies (uncomment as needed)\n",
    "# If you're on Colab, also: !apt -y install -qq libstdc++6\n",
    "# %pip install -q langchain langchain-community langchain-text-splitters langchain-core\n",
    "# %pip install -q faiss-cpu\n",
    "# %pip install -q sentence-transformers\n",
    "# %pip install -q ragas datasets evaluate # (optional, for evaluation section)\n",
    "# %pip install -q langchain-ollama  # for local LLMs via Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499632af",
   "metadata": {},
   "source": [
    "## RAG in one picture\n",
    "\n",
    "**RAG = Retrieval + Generation.**  \n",
    "1) **Indexing**: Load data → chunk → embed → store in a vector DB.  \n",
    "2) **Retrieval**: Convert user query → retrieve relevant chunks.  \n",
    "3) **Generation**: Feed retrieved chunks + query to an LLM with a grounded prompt → **answer with citations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea198cb",
   "metadata": {},
   "source": [
    "## Dataset (Toy Corpus)\n",
    "\n",
    "For demo, we'll use a small, self‑contained corpus so everything runs offline:\n",
    "- `banking_faq.txt` — toy banking/domain knowledge\n",
    "- `ml_rag_notes.txt` — small RAG notes\n",
    "\n",
    "You can replace these with PDFs, web pages, markdown, or your own folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny local corpus so the notebook is self-contained\n",
    "banking_faq = \"\"\"\n",
    "Q: What is two-factor authentication (2FA)?\n",
    "A: A security process that requires two distinct forms of identification: something you know (password) and something you have (OTP/device).\n",
    "\n",
    "Q: What is NEFT?\n",
    "A: National Electronic Funds Transfer, an Indian retail payment system for one-to-one money transfers between bank accounts.\n",
    "\n",
    "Q: What is data at rest vs data in transit?\n",
    "A: Data at rest is stored data (e.g., on disk); data in transit moves across networks. Both should be protected with encryption and access controls.\n",
    "\"\"\"\n",
    "\n",
    "ml_rag_notes = \"\"\"\n",
    "Retrieval Augmented Generation (RAG) combines information retrieval with text generation.\n",
    "Core steps: indexing (load, split, embed, store), retrieval (top-k, hybrid, filters),\n",
    "and generation (prompt with retrieved context).\n",
    "Advanced: query rewriting, multi-vector retrievers, reranking, stuffing vs map-reduce,\n",
    "and evaluation (faithfulness, answer relevancy, context precision/recall).\n",
    "\"\"\"\n",
    "\n",
    "with open(\"banking_faq.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(banking_faq)\n",
    "with open(\"ml_rag_notes.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(ml_rag_notes)\n",
    "\n",
    "print(\"Created banking_faq.txt and ml_rag_notes.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed28c8",
   "metadata": {},
   "source": [
    "## 1) Ingestion & Chunking\n",
    "\n",
    "We’ll load plain‑text files and chunk them with `RecursiveCharacterTextSplitter`.  \n",
    "Chunk size/overlap matters: **too big** → slow, irrelevant; **too small** → lose context. Start with 500–1,000 tokens (or ~1,500–3,000 chars) and **tune**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320293da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load\n",
    "paths = [\"banking_faq.txt\", \"ml_rag_notes.txt\"]\n",
    "docs = []\n",
    "for p in paths:\n",
    "    docs.extend(TextLoader(p, encoding=\"utf-8\").load())\n",
    "\n",
    "# Split\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "len(docs), len(chunks), chunks[0].page_content[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe6338",
   "metadata": {},
   "source": [
    "## 2) Embeddings & Vector Store (FAISS)\n",
    "\n",
    "We’ll use **HuggingFace** MiniLM embeddings (good quality & local) and **FAISS** for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8af0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Embeddings model (downloads on first use)\n",
    "emb_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build the index\n",
    "vectorstore = FAISS.from_documents(chunks, emb_model)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e5509",
   "metadata": {},
   "source": [
    "## 3) LLM Setup\n",
    "\n",
    "We’ll default to **Ollama** (e.g., `mistral`, `llama3`) to stay local.  \n",
    "If you don’t use Ollama, switch to `ChatOpenAI` or another provider by replacing the imports and the `llm` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose ONE of the below.\n",
    "\n",
    "# --- A) Local LLM via Ollama (recommended for offline) ---\n",
    "try:\n",
    "    from langchain_ollama import ChatOllama\n",
    "    llm = ChatOllama(model=\"mistral\")  # or \"llama3\", \"qwen2\", etc. in your Ollama\n",
    "    USING_OLLAMA = True\n",
    "except Exception as e:\n",
    "    print(\"Ollama not available. Falling back to a no-op mock LLM.\")\n",
    "    USING_OLLAMA = False\n",
    "    class MockLLM:\n",
    "        def invoke(self, msgs):\n",
    "            return type(\"Msg\", (), {\"content\": \"MockLLM: Provide a real LLM (Ollama/OpenAI/Anthropic/etc.)\"})\n",
    "    llm = MockLLM()\n",
    "\n",
    "# --- B) Alternative: OpenAI (uncomment and set env var OPENAI_API_KEY) ---\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7177f",
   "metadata": {},
   "source": [
    "## 4) Prompting Strategy\n",
    "\n",
    "We’ll use a **stuff** prompt template: instructions + retrieved context + question.  \n",
    "Best practices:\n",
    "- Keep instructions **clear and firm** (cite sources, don't fabricate).\n",
    "- Add **formatting** (e.g., bullet points, JSON if needed).\n",
    "- Provide **fallback** if context is insufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc18478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer using ONLY the provided context. \"\n",
    "               \"Cite sources as (S1), (S2), etc. If the answer isn't in context, say you don't know.\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer:\")\n",
    "])\n",
    "RAG_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623403b2",
   "metadata": {},
   "source": [
    "## 5) Build the RAG Chain (Retrieve → Format → Generate)\n",
    "\n",
    "We’ll:\n",
    "1) Retrieve top‑k chunks  \n",
    "2) Format them into a single context string  \n",
    "3) Call the LLM with our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77eb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        formatted.append(f\"(S{i})\\n{d.page_content.strip()}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# Chain: input question -> retrieve docs -> format -> prompt -> llm\n",
    "rag_chain = (\n",
    "    {\"docs\": retriever | RunnableLambda(lambda x: x), \"question\": RunnablePassthrough()}\n",
    "    | {\"context\": itemgetter(\"docs\") | RunnableLambda(format_docs), \"question\": itemgetter(\"question\")}\n",
    "    | RAG_PROMPT\n",
    "    | RunnableLambda(lambda p: llm.invoke(p) if hasattr(llm, \"invoke\") else llm(p))\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke(\"What is NEFT and how is it used?\")\n",
    "print(getattr(response, \"content\", response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151c49d",
   "metadata": {},
   "source": [
    "## 6) Return Sources Separately (for UI apps)\n",
    "\n",
    "Often you’ll want the answer **and** the underlying source docs to show citations or links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480382f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str, k: int = 4):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    ctx = format_docs(docs)\n",
    "    msg = RAG_PROMPT.invoke({\"question\": question, \"context\": ctx})\n",
    "    out = llm.invoke(msg) if hasattr(llm, \"invoke\") else llm(msg)\n",
    "    return getattr(out, \"content\", out), docs\n",
    "\n",
    "answer, sources = ask(\"Explain data at rest vs data in transit.\")\n",
    "print(\"ANSWER:\\n\", answer, \"\\n\")\n",
    "print(\"SOURCES:\")\n",
    "for i, d in enumerate(sources, 1):\n",
    "    print(f\"  (S{i}) {d.metadata.get('source')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c637e",
   "metadata": {},
   "source": [
    "## 7) Query Transformations (Better Retrieval)\n",
    "\n",
    "Techniques:\n",
    "- **Rewriting** (e.g., condense follow‑ups into standalone queries)\n",
    "- **HyDE** (generate hypothetical answer → embed → retrieve)\n",
    "- **Multi‑query** (expand into several paraphrases → merge results)\n",
    "\n",
    "Below is a simple **multi‑query** example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "multiquery_template = PromptTemplate.from_template(\n",
    "    \"Generate 3 diverse search queries that rephrase: '{question}'. Return one per line.\"\n",
    ")\n",
    "\n",
    "def multiquery_retrieve(question: str, top_k: int = 3, per_query_k: int = 3):\n",
    "    # Expand\n",
    "    if hasattr(llm, \"invoke\"):\n",
    "        gen = llm.invoke(multiquery_template.format(question=question))\n",
    "        queries = [q.strip(\"-• \").strip() for q in getattr(gen, \"content\", str(gen)).splitlines() if q.strip()]\n",
    "    else:\n",
    "        queries = [question, question + \" details\", \"explain \" + question]\n",
    "\n",
    "    # Retrieve per query and merge\n",
    "    seen = set()\n",
    "    merged = []\n",
    "    for q in queries[:top_k]:\n",
    "        hits = retriever.get_relevant_documents(q)[:per_query_k]\n",
    "        for h in hits:\n",
    "            key = (h.page_content, h.metadata.get(\"source\"))\n",
    "            if key not in seen:\n",
    "                seen.add(key); merged.append(h)\n",
    "    return merged\n",
    "\n",
    "q = \"What is RAG?\"\n",
    "docs_mq = multiquery_retrieve(q)\n",
    "print(\"Retrieved\", len(docs_mq), \"unique chunks via multiquery.\")\n",
    "print(docs_mq[0].page_content[:160])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18e641",
   "metadata": {},
   "source": [
    "## 8) Lightweight Re‑ranking (Optional)\n",
    "\n",
    "If you have a cross‑encoder (e.g., `cross-encoder/ms-marco-MiniLM-L-6-v2`), you can **re‑rank** top‑k docs by relevance score.  \n",
    "Below is **optional** pseudocode — uncomment if you install `sentence-transformers` cross encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: simple reranker with a cross-encoder\n",
    "# from sentence_transformers import CrossEncoder\n",
    "# reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# def rerank(question: str, docs):\n",
    "#     pairs = [(question, d.page_content) for d in docs]\n",
    "#     scores = reranker.predict(pairs)\n",
    "#     ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "#     return [d for d, s in ranked]\n",
    "\n",
    "# # Example usage:\n",
    "# top = retriever.get_relevant_documents(\"What is NEFT?\")\n",
    "# top = rerank(\"What is NEFT?\", top)\n",
    "# format_docs(top[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a47e90",
   "metadata": {},
   "source": [
    "## 9) Build a Simple RAG Function (Reusable)\n",
    "\n",
    "Wraps everything: retrieval (+optional multiquery), formatting, generation, and returns answer + sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be04db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(question: str, use_multiquery: bool = False, k: int = 4):\n",
    "    docs = multiquery_retrieve(question, top_k=3, per_query_k=3) if use_multiquery else retriever.get_relevant_documents(question)\n",
    "    ctx = format_docs(docs[:k])\n",
    "    msg = RAG_PROMPT.invoke({\"question\": question, \"context\": ctx})\n",
    "    out = llm.invoke(msg) if hasattr(llm, \"invoke\") else llm(msg)\n",
    "    return getattr(out, \"content\", out), docs[:k]\n",
    "\n",
    "ans, src = rag_answer(\"How does two-factor authentication work?\", use_multiquery=True)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59fcffa",
   "metadata": {},
   "source": [
    "## 10) Evaluation Basics (RAGAS / Heuristics)\n",
    "\n",
    "**Why evaluate RAG?** To track: answer relevance, faithfulness (groundedness), context relevance/precision/recall.\n",
    "\n",
    "- **RAGAS** can compute metrics using an LLM judge (requires an API/LLM).  \n",
    "- **Heuristics** (cheap): Answer contains keywords present in the retrieved context; exact‑match to FAQ; length/overlap checks.\n",
    "\n",
    "We include a small, LLM‑free heuristic as a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d163d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def simple_overlap_score(answer: str, docs) -> float:\n",
    "    ctx = \" \".join(d.page_content for d in docs).lower()\n",
    "    toks = set(re.findall(r\"[a-zA-Z0-9]+\", answer.lower()))\n",
    "    toks = {t for t in toks if len(t) > 3}\n",
    "    if not toks:\n",
    "        return 0.0\n",
    "    hit = sum(1 for t in toks if t in ctx)\n",
    "    return hit / len(toks)\n",
    "\n",
    "test_q = \"What is data at rest vs data in transit?\"\n",
    "ans, src = rag_answer(test_q, use_multiquery=False)\n",
    "print(\"ANSWER:\", ans[:200], \"...\")\n",
    "print(\"Overlap score (0-1):\", round(simple_overlap_score(ans, src), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5eed91",
   "metadata": {},
   "source": [
    "## 11) Production Considerations & Advanced Patterns\n",
    "\n",
    "- **Hybrid retrieval**: BM25 (sparse) + dense embeddings → better recall.  \n",
    "- **Multi‑vector**: store titles, summaries, key phrases alongside body embeddings.  \n",
    "- **Query routing**: choose retriever by domain/namespace.  \n",
    "- **Structured output**: ask the LLM to produce JSON with fields (answer, citations).  \n",
    "- **Safety**: protect PII, apply redaction, add guardrails.  \n",
    "- **Caching**: store embeddings, responses (LSH cache).  \n",
    "- **Observability**: log traces (LangSmith), track latency, failure modes.  \n",
    "- **Chunking strategies**: by headings/semantic breaks; adaptive chunking.  \n",
    "- **Freshness**: add **web search** or a **SQL retriever** for live data.  \n",
    "- **Agents**: allow tools (search/DB) for retrieval beyond pure vector stores.  \n",
    "- **Security**: document‑level/row‑level ACLs; encrypt at rest & in transit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d11d8",
   "metadata": {},
   "source": [
    "## 12) Full Minimal App (Console)\n",
    "\n",
    "A tiny loop to ask questions. In real apps, use FastAPI/Gradio/Streamlit and show sources inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecbcd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    print(\"RAG chat. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        q = input(\"\\nYou: \").strip()\n",
    "        if q.lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "        ans, src = rag_answer(q, use_multiquery=True)\n",
    "        print(\"\\nAssistant:\", ans)\n",
    "        print(\"\\nSources:\")\n",
    "        for i, d in enumerate(src, 1):\n",
    "            print(f\"  (S{i}) {d.metadata.get('source')}\")\n",
    "\n",
    "# Uncomment to try:\n",
    "# chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1882c",
   "metadata": {},
   "source": [
    "## 13) Alternatives / Related Techniques\n",
    "\n",
    "- **Fusion‑in‑Decoder (FiD)**: encode multiple docs and let the decoder attend across them.  \n",
    "- **ColBERT / Late Interaction**: fine retrieval with token‑level interactions.  \n",
    "- **GraphRAG**: build a knowledge graph and retrieve subgraphs as context.  \n",
    "- **Toolformer / Agents**: tool‑use during generation instead of pre‑retrieval.  \n",
    "- **Index‑aware prompting**: train prompts to match chunking & metadata.  \n",
    "- **Knowledge distillation**: pre‑compute Q/A pairs; train a domain‑LLM or a reranker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa36fd",
   "metadata": {},
   "source": [
    "## 14) What a Data Scientist Should Know (Checklist)\n",
    "\n",
    "- **Data**: formats, loaders, cleaning, PII redaction, deduplication, canonicalization.  \n",
    "- **Embeddings**: model choice, dimensionality, normalization, drift monitoring.  \n",
    "- **Index**: FAISS/HNSW params (nlist, nprobe/efSearch), ANN recall/latency trade‑offs.  \n",
    "- **Retrieval**: k, filters, re‑ranking, query rewrite, hybrid search.  \n",
    "- **Prompting**: system instructions, style, output schema, refusal policy.  \n",
    "- **Quality**: eval sets, AB tests, RAGAS/G-Eval; track hallucinations.  \n",
    "- **Ops**: concurrency, caching, cold‑start, containerization, CI/CD.  \n",
    "- **Security**: encryption (at rest/in transit), access control, audit logs.  \n",
    "- **Cost & Latency**: embeddings batch, quantized LLMs, streaming.  \n",
    "- **Compliance**: data residency, retention, legal holds, consent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd134559",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### You're done!\n",
    "Try swapping the corpus, embeddings, and LLM to match your real use case.  \n",
    "Integrate a web UI and display sources as expandable snippets with highlights."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
