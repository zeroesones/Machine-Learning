{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234e117e",
   "metadata": {},
   "source": [
    "# Universal Data Cleaning & Preprocessing Playbook (Do-These-First)\n",
    "\n",
    "Below are **12 common steps** you can apply to most tabular datasets.  \n",
    "Each step includes: *what/why*, the **go-to Python function(s)**, a **template**, and then **exercises** for self-practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33180ad7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### How to use this section during the workshop\n",
    "\n",
    "1. Skim each step to recall the **purpose** and the **function to use**.  \n",
    "2. Run the **template** cell to see the pattern.  \n",
    "3. Attempt the **exercises** right below, then compare with the instructor's solution later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83615675",
   "metadata": {},
   "source": [
    "> **Playbook Bootstrap**  \n",
    "> If `df` is not yet defined, this cell creates a small synthetic dataset so you can run the exercises immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda43b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert \"df\" in globals() and isinstance(df, pd.DataFrame)\n",
    "except Exception:\n",
    "    import numpy as np, pandas as pd\n",
    "    rng = np.random.default_rng(7)\n",
    "    n = 500\n",
    "    df = pd.DataFrame({\n",
    "        \"age\": rng.integers(18, 70, size=n),\n",
    "        \"income\": rng.normal(60000, 15000, size=n).round(0),\n",
    "        \"tenure_years\": rng.exponential(3, size=n).round(2),\n",
    "        \"segment\": rng.choice([\"A\",\"B\",\"C\"], size=n, p=[0.5,0.3,0.2]),\n",
    "        \"region\": rng.choice([\"North\",\"South\",\"East\",\"West\"], size=n),\n",
    "        \"joined_on\": pd.to_datetime(\"2020-01-01\") + pd.to_timedelta(rng.integers(0, 1500, size=n), unit=\"D\"),\n",
    "        \"notes\": rng.choice([\"loves discounts\", \"calls often\", \"email only\", \"no preference\"], size=n),\n",
    "    })\n",
    "    # inject issues\n",
    "    df.loc[rng.choice(n, 20, replace=False), \"income\"] = np.nan\n",
    "    df.loc[rng.choice(n, 10, replace=False), \"age\"] = None\n",
    "    df.loc[rng.choice(n, 5, replace=False), \"tenure_years\"] = 99\n",
    "    # simple target\n",
    "    df[\"churn\"] = ((df[\"income\"].fillna(50000) < 55000) | (df[\"tenure_years\"] < 1.5) | (df[\"segment\"]==\"C\")).astype(int)\n",
    "    print(\"Created synthetic df with shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d453f",
   "metadata": {},
   "source": [
    "## 1) Schema & Dtype Validation\n",
    "\n",
    "**Why:** Wrong dtypes (numbers as strings, dates as objects) break downstream logic.  \n",
    "**Go-to:** `pandas.to_numeric`, `pandas.to_datetime`, `astype`, `DataFrame.convert_dtypes()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ffc03",
   "metadata": {},
   "source": [
    "### Storytime: Schema & Dtype Validation  \n",
    "Imagine opening a mysterious chest of data treasures. Some jewels are labeled correctly (`int`, `float`, `datetime`), but others are mislabeled—numbers stored as strings or dates written like riddles.  \n",
    "**Why do this step?** Because if you don’t, downstream algorithms may stumble trying to add `\"2000\"` to `2000.0`.  \n",
    "**Risk of skipping:** Models may silently treat numbers as text, miss calculations, or break when handling nulls. Always check the labels on your jewels before trading them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE: fix dtypes\n",
    "# df = df.copy()\n",
    "for col in [\"age\", \"income\", \"tenure_years\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "df[\"joined_on\"] = pd.to_datetime(df[\"joined_on\"], errors=\"coerce\")\n",
    "# df = df.convert_dtypes()  # optional newer dtype inference\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839255f",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Convert any object-numeric columns to numeric with `errors=\"coerce\"` and report how many NaNs were created.\n",
    "- Parse a date column; add `day`, `month`, `year` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your attempt here\n",
    "# Example: count NaNs introduced by coercion\n",
    "# before_nans = df.isna().sum()\n",
    "# df[\"income\"] = pd.to_numeric(df[\"income\"], errors=\"coerce\")\n",
    "# after_nans = df.isna().sum()\n",
    "# print(after_nans - before_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6af088",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 1 (Schema & Dtypes)</b></summary>\n",
    "\n",
    "- Coerce problematic numeric strings to numbers using `pd.to_numeric(errors=\"coerce\")` and report NaN deltas.  \n",
    "- Parse datetimes and derive `day`, `month`, `year`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NaNs introduced by coercion on candidate numeric cols\n",
    "cand = [\"age\", \"income\", \"tenure_years\"]\n",
    "before = df[cand].isna().sum()\n",
    "for col in cand:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "after = df[cand].isna().sum()\n",
    "print(\"NaNs introduced by coercion:\\n\", (after - before))\n",
    "\n",
    "# Parse joined_on and add parts\n",
    "df[\"joined_on\"] = pd.to_datetime(df[\"joined_on\"], errors=\"coerce\")\n",
    "df[\"day\"] = df[\"joined_on\"].dt.day\n",
    "df[\"month\"] = df[\"joined_on\"].dt.month\n",
    "df[\"year\"] = df[\"joined_on\"].dt.year\n",
    "df[[\"joined_on\",\"day\",\"month\",\"year\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f1796",
   "metadata": {},
   "source": [
    "## 2) Text Normalization (Whitespace/Case)\n",
    "\n",
    "**Why:** Hidden spaces and case inconsistencies create artificial categories.  \n",
    "**Go-to:** `str.strip`, `str.lower`, `str.replace`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23ab67",
   "metadata": {},
   "source": [
    "### Storytime: Text Normalization  \n",
    "Think of your dataset as a classroom roll call. `\"North \"` and `\"north\"` are the same student arriving with different uniforms.  \n",
    "**Why do this step?** To ensure fairness—grouping all variants into one true identity.  \n",
    "**Risk of skipping:** Your model believes `\"North\"` and `\"north\"` are two separate regions, splitting loyalty and weakening insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024af0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE: clean string columns\n",
    "str_cols = df.select_dtypes(include=\"object\").columns\n",
    "for c in str_cols:\n",
    "    df[c] = df[c].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True).str.lower()\n",
    "df[str_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5995d",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Standardize casing in `region/segment` and validate unique values before/after.\n",
    "- Remove double spaces inside any free-text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# uniques_before = df[\"region\"].unique()\n",
    "# df[\"region\"] = df[\"region\"].str.strip().str.lower()\n",
    "# uniques_after = df[\"region\"].unique()\n",
    "# print(uniques_before, \"->\", uniques_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01861e20",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 2 (Text Normalization)</b></summary>\n",
    "\n",
    "- Standardize whitespace and case; verify unique values changed as expected.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.select_dtypes(include=\"object\").columns:\n",
    "    before_uniques = df[c].unique()\n",
    "    df[c] = df[c].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True).str.lower()\n",
    "    after_uniques = df[c].unique()\n",
    "print(\"Normalized object columns. Example 'region' uniques:\", df[\"region\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b4628",
   "metadata": {},
   "source": [
    "## 3) De-duplication\n",
    "\n",
    "**Why:** Duplicate rows bias metrics and inflate data size.  \n",
    "**Go-to:** `DataFrame.duplicated`, `drop_duplicates`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308b743",
   "metadata": {},
   "source": [
    "### Storytime: De-duplication  \n",
    "Imagine counting villagers in a town square. If you count the same person twice, your population size grows unfairly.  \n",
    "**Why do this step?** Duplicates inflate statistics, bias averages, and mislead predictions.  \n",
    "**Risk of skipping:** You may design resources (or models) based on phantom citizens, leading to wasted effort or wrong conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE: remove duplicates, track impact\n",
    "n_before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "print(\"Removed\", n_before - len(df), \"duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b62787b",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Count duplicates per subset of columns (e.g., `[\"age\",\"region\",\"segment\"]`).  \n",
    "- Keep the **last** occurrence of duplicates and compare counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# dup_mask = df.duplicated(subset=[\"age\",\"region\",\"segment\"], keep=False)\n",
    "# df_dups = df[dup_mask]\n",
    "# df_last = df.drop_duplicates(subset=[\"age\",\"region\",\"segment\"], keep=\"last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a9999",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 3 (De-duplication)</b></summary>\n",
    "\n",
    "- Identify duplicates by subset and compare keeping first vs last.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [\"age\",\"region\",\"segment\"]\n",
    "dup_mask = df.duplicated(subset=subset, keep=False)\n",
    "dups = df[dup_mask]\n",
    "print(\"Duplicate groups (subset):\", dups.shape[0])\n",
    "\n",
    "keep_first = df.drop_duplicates(subset=subset, keep=\"first\").shape[0]\n",
    "keep_last  = df.drop_duplicates(subset=subset, keep=\"last\").shape[0]\n",
    "print(\"Rows if keep=first:\", keep_first, \"| keep=last:\", keep_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb34eb",
   "metadata": {},
   "source": [
    "## 4) Missing-Value Strategy\n",
    "\n",
    "**Why:** Models require complete matrices; missingness also holds signal.  \n",
    "**Go-to:** `SimpleImputer`, `KNNImputer`, create \"Missing\" category for categoricals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc14696",
   "metadata": {},
   "source": [
    "### Storytime: Missing Values  \n",
    "In an ancient scroll, some words have faded away. Do you ignore them, guess the missing letters, or fill them with placeholders?  \n",
    "**Why do this step?** Models can’t handle blanks—they need complete sentences.  \n",
    "**Risk of skipping:** Leaving gaps causes models to crash or hallucinate patterns, distorting results. The way you fill the blanks shapes the story told by the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE: numeric/categorical imputers\n",
    "num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "df.isna().sum().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fad326",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Compare `median` vs `KNNImputer` on numeric columns—measure effect on model F1.  \n",
    "- Add a boolean indicator column for \"was_missing\" for a selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# from sklearn.metrics import f1_score\n",
    "# # Fit model with median vs KNN and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9597d",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 4 (Missing Values)</b></summary>\n",
    "\n",
    "- Compare median vs KNN imputers by F1 on a baseline model.  \n",
    "- Add missingness indicator.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "X = df.drop(columns=[\"churn\"])\n",
    "y = df[\"churn\"].astype(int)\n",
    "\n",
    "num_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
    "\n",
    "# Add a missingness indicator for income\n",
    "X[\"income_was_missing\"] = X[\"income\"].isna().astype(int)\n",
    "\n",
    "pre_median = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols)\n",
    "])\n",
    "\n",
    "pre_knn = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", KNNImputer(n_neighbors=5)), (\"sc\", StandardScaler())]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols)\n",
    "])\n",
    "\n",
    "for name, pre in [(\"median\", pre_median), (\"knn\", pre_knn)]:\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"clf\", LogisticRegression(max_iter=300))])\n",
    "    scores = cross_val_score(pipe, X, y, cv=5, scoring=\"f1\")\n",
    "    print(f\"{name} imputer CV F1 mean:\", scores.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e79fc23",
   "metadata": {},
   "source": [
    "## 5) Outlier Handling\n",
    "\n",
    "**Why:** Extremes can distort means and model coefficients.  \n",
    "**Go-to:** Quantile capping (`clip`), `LocalOutlierFactor` (inspection), `IsolationForest`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453adb08",
   "metadata": {},
   "source": [
    "### Storytime: Outliers  \n",
    "Picture a feast where one guest eats 100 plates while everyone else eats 3. That single guest distorts the banquet’s average.  \n",
    "**Why do this step?** Outliers can dominate scales, shift means, and mislead models.  \n",
    "**Risk of skipping:** Your model may believe everyone is a glutton, overestimating costs or risks. Handling outliers keeps the feast realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE: winsorize/cap and flag with LOF\n",
    "cap = df[\"tenure_years\"].quantile(0.99)\n",
    "df[\"tenure_years\"] = df[\"tenure_years\"].clip(upper=cap)\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.02)\n",
    "is_inlier = lof.fit_predict(df[num_cols]) == 1\n",
    "print(\"Flagged outliers:\", (~is_inlier).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9239b7",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Try 95th vs 99th percentile caps and visualize histograms side-by-side.  \n",
    "- Drop LOF-flagged outliers and compare CV metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ab811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# df95 = df.copy()\n",
    "# df95[\"tenure_years\"] = df95[\"tenure_years\"].clip(upper=df95[\"tenure_years\"].quantile(0.95))\n",
    "# ax = df[\"tenure_years\"].hist(bins=30)\n",
    "# ax = df95[\"tenure_years\"].hist(bins=30, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e4aa8",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 5 (Outliers)</b></summary>\n",
    "\n",
    "- Compare caps at different percentiles visually and quantify effect.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df95 = df.copy()\n",
    "df99 = df.copy()\n",
    "df95[\"tenure_years\"] = df95[\"tenure_years\"].clip(upper=df95[\"tenure_years\"].quantile(0.95))\n",
    "df99[\"tenure_years\"] = df99[\"tenure_years\"].clip(upper=df99[\"tenure_years\"].quantile(0.99))\n",
    "\n",
    "df[\"tenure_years\"].hist(bins=30); plt.title(\"Original tenure_years\"); plt.show()\n",
    "df95[\"tenure_years\"].hist(bins=30); plt.title(\"Capped at 95th\"); plt.show()\n",
    "df99[\"tenure_years\"].hist(bins=30); plt.title(\"Capped at 99th\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35476479",
   "metadata": {},
   "source": [
    "## 6) Business-Rule Validation\n",
    "\n",
    "**Why:** Guardrails against bad upstream data.  \n",
    "**Go-to:** `assert`, bounded checks, membership tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b865737",
   "metadata": {},
   "source": [
    "### Storytime: Business Rules  \n",
    "Imagine a guard at the city gate checking travelers: no child under 18 may enter the tavern.  \n",
    "**Why do this step?** Business rules enforce domain reality—ages, ranges, memberships.  \n",
    "**Risk of skipping:** A 5-year-old might be counted as a loan applicant or a negative income might sneak into salary records, shattering trust and logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e68bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE: assertions\n",
    "assert (df[\"age\"] >= 0).all(), \"Age must be non-negative\"\n",
    "assert df[\"segment\"].isin([\"a\",\"b\",\"c\"]).all(), \"Unexpected segment values\"\n",
    "print(\"Business rules passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c731722",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Add a rule for `income` minimum; fail fast if violated.  \n",
    "- Create a function `validate_schema(df)` that returns a dict of failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8df2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# def validate_schema(df):\n",
    "#     errors = {}\n",
    "#     if not (df[\"income\"] >= 0).all():\n",
    "#         errors[\"income\"] = \"negative values found\"\n",
    "#     return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36766a4d",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 6 (Business Rules)</b></summary>\n",
    "\n",
    "- Validate ranges; return a dictionary of failures.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema(df):\n",
    "    errors = {}\n",
    "    if not (df[\"income\"].fillna(0) >= 0).all():\n",
    "        errors[\"income\"] = \"negative or invalid values\"\n",
    "    if not df[\"segment\"].isin([\"a\",\"b\",\"c\"]).all():\n",
    "        errors[\"segment\"] = \"unexpected categories present\"\n",
    "    return errors\n",
    "\n",
    "validate_schema(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb0954f",
   "metadata": {},
   "source": [
    "## 7) Categorical Encoding\n",
    "\n",
    "**Why:** Models expect numeric inputs.  \n",
    "**Go-to:** `OneHotEncoder` for nominal; `OrdinalEncoder` for ordered categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0187a7",
   "metadata": {},
   "source": [
    "### Storytime: Categorical Encoding  \n",
    "Think of categories as different languages—French, Hindi, Japanese. Models only understand numbers.  \n",
    "**Why do this step?** Encoding translates human-friendly categories into model-friendly digits.  \n",
    "**Risk of skipping:** The model hears gibberish—strings it cannot process. Worse, wrong encoding (like ordinals where none exist) tells the model a false story of rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa86c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cat_cols = [\"region\",\"segment\"] if set([\"region\",\"segment\"]).issubset(df.columns) else df.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144c77f",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Use `OrdinalEncoder` with an explicit order for `segment` and compare model performance vs one-hot.  \n",
    "- Measure dimensionality growth after one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ord = OrdinalEncoder(categories=[[\"c\",\"b\",\"a\"]])\n",
    "# df[\"segment_ord\"] = ord.fit_transform(df[[\"segment\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc660988",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 7 (Categorical Encoding)</b></summary>\n",
    "\n",
    "- Ordinal-encode `segment` with explicit order and inspect distribution.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "if \"segment\" in df.columns:\n",
    "    ord = OrdinalEncoder(categories=[[\"c\",\"b\",\"a\"]])\n",
    "    df[\"segment_ord\"] = ord.fit_transform(df[[\"segment\"]])\n",
    "    print(df[\"segment_ord\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c67a4",
   "metadata": {},
   "source": [
    "## 8) Scaling Numerical Features\n",
    "\n",
    "**Why:** Many models are sensitive to feature scales.  \n",
    "**Go-to:** `StandardScaler`, `MinMaxScaler`, `RobustScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef00e7",
   "metadata": {},
   "source": [
    "### Storytime: Scaling  \n",
    "Imagine runners in a race: one measured in meters, another in kilometers. Who seems faster?  \n",
    "**Why do this step?** Scaling puts all features on comparable scales, ensuring fair competition.  \n",
    "**Risk of skipping:** Models that rely on distances (kNN, SVM, PCA) will crown the wrong champion, misjudging importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4117479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[num_cols] = scaler.fit_transform(df_scaled[num_cols])\n",
    "df_scaled[num_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a11ea",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Compare `StandardScaler` vs `RobustScaler` on outlier-heavy data.  \n",
    "- Show effect on a k-NN classifier’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b26b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# scaler2 = RobustScaler()\n",
    "# df_scaled2 = df.copy()\n",
    "# df_scaled2[num_cols] = scaler2.fit_transform(df_scaled2[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d5a4b",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 8 (Scaling)</b></summary>\n",
    "\n",
    "- Compare Standard vs Robust scaling effects.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "ss = StandardScaler().fit_transform(df[num_cols])\n",
    "rs = RobustScaler().fit_transform(df[num_cols])\n",
    "print(\"StdScaler mean (approx 0):\", ss.mean().round(4), \" | Robust median ~0 (not printed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb4ad26",
   "metadata": {},
   "source": [
    "## 9) Datetime Feature Engineering\n",
    "\n",
    "**Why:** Raw datetimes need transformation to become useful.  \n",
    "**Go-to:** `.dt.year`, `.dt.month`, timedeltas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b691ed6",
   "metadata": {},
   "source": [
    "### Storytime: Datetime Features  \n",
    "Your dataset has birthdays, but the real magic is in age.  \n",
    "**Why do this step?** Raw timestamps hide patterns; extracting years, months, and durations uncovers trends like seasonality and recency.  \n",
    "**Risk of skipping:** The model stares at raw dates with no clue how to compare `\"2022-03-01\"` to `\"2021-09-15\"`, losing temporal wisdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec05825",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.to_datetime(df[\"joined_on\"], errors=\"coerce\")\n",
    "df[\"join_year\"] = dt.dt.year\n",
    "df[\"join_month\"] = dt.dt.month\n",
    "df[\"tenure_days\"] = (pd.Timestamp.today().normalize() - dt).dt.days\n",
    "df[[\"join_year\",\"join_month\",\"tenure_days\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c59396",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Create `is_weekend`, `quarter`, and `days_since_year_start`.  \n",
    "- Bucketize `tenure_days` into 5 quantile bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# df[\"quarter\"] = dt.dt.quarter\n",
    "# df[\"is_weekend\"] = dt.dt.weekday >= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2545b6c",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 9 (Datetime)</b></summary>\n",
    "\n",
    "- Weekend and quarter flags; days since year start.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff40dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.to_datetime(df[\"joined_on\"], errors=\"coerce\")\n",
    "df[\"quarter\"] = dt.dt.quarter\n",
    "df[\"is_weekend\"] = (dt.dt.weekday >= 5).astype(int)\n",
    "df[\"days_since_year_start\"] = (dt - pd.to_datetime(dt.dt.year.astype(str) + \"-01-01\")).dt.days\n",
    "df[[\"quarter\",\"is_weekend\",\"days_since_year_start\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26b7bf",
   "metadata": {},
   "source": [
    "## 10) Binning & Threshold Features\n",
    "\n",
    "**Why:** Capture non-linear thresholds with simpler models.  \n",
    "**Go-to:** `KBinsDiscretizer`, `pd.cut`, `pd.qcut`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c7ea3",
   "metadata": {},
   "source": [
    "### Storytime: Binning  \n",
    "Think of exam scores grouped into grades A, B, C. Small differences vanish, but broad categories reveal clearer groups.  \n",
    "**Why do this step?** Binning captures thresholds and simplifies non-linear relationships.  \n",
    "**Risk of skipping:** Subtle patterns stay hidden; the model may struggle with raw continuous values where thresholds matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593be8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "kb = KBinsDiscretizer(n_bins=4, encode=\"ordinal\", strategy=\"quantile\")\n",
    "df[\"income_bin\"] = kb.fit_transform(df[[\"income\"]]).astype(int)\n",
    "df[\"age_bin\"] = kb.fit_transform(df[[\"age\"]]).astype(int)\n",
    "df[[\"income\",\"income_bin\",\"age\",\"age_bin\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297240d7",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Compare `pd.cut` vs `pd.qcut` for `income`; inspect bin sizes.  \n",
    "- Train a simple logistic regression on binned vs raw features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# df[\"income_qcut\"] = pd.qcut(df[\"income\"], q=4, duplicates=\"drop\")\n",
    "# df[\"income_cut\"] = pd.cut(df[\"income\"], bins=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22731780",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 10 (Binning)</b></summary>\n",
    "\n",
    "- Compare equal-width vs quantile bins.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"income_qcut\"] = pd.qcut(df[\"income\"], q=4, duplicates=\"drop\")\n",
    "df[\"income_cut\"] = pd.cut(df[\"income\"], bins=4)\n",
    "print(\"qcut sizes:\\n\", df[\"income_qcut\"].value_counts().sort_index())\n",
    "print(\"cut sizes:\\n\", df[\"income_cut\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b839f",
   "metadata": {},
   "source": [
    "## 11) Feature Selection (Filter/Wrappers)\n",
    "\n",
    "**Why:** Reduce noise, speed up training, improve generalization.  \n",
    "**Go-to:** `VarianceThreshold`, `SelectKBest(mutual_info_)`, model-based (`RFECV`, `SelectFromModel`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6aa976",
   "metadata": {},
   "source": [
    "### Storytime: Feature Selection  \n",
    "Picture a storyteller with 100 characters. Too many side plots distract from the hero’s journey.  \n",
    "**Why do this step?** Feature selection removes noisy, irrelevant features, sharpening focus.  \n",
    "**Risk of skipping:** The model wastes time on background noise, overfits, and forgets the main storyline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "X = df.drop(columns=[c for c in [\"churn\"] if c in df.columns])\n",
    "y = df[\"churn\"].astype(int) if \"churn\" in df.columns else (df.iloc[:, -1] > df.iloc[:, -1].median()).astype(int)\n",
    "\n",
    "vt = VarianceThreshold(0.0)\n",
    "X_vt = vt.fit_transform(X.select_dtypes(include=[\"number\"]))\n",
    "skb = SelectKBest(score_func=mutual_info_classif, k=min(10, X_vt.shape[1]))\n",
    "X_sel = skb.fit_transform(X_vt, y)\n",
    "print(\"Selected features:\", skb.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943b6f8",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Plot top-10 MI scores as a bar chart.  \n",
    "- Try `RFECV` with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba828c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# rfecv = RFECV(LogisticRegression(max_iter=300), cv=5, scoring=\"f1\")\n",
    "# rfecv.fit(X.select_dtypes(include=[\"number\"]), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdd265",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 11 (Feature Selection)</b></summary>\n",
    "\n",
    "- Plot MI scores and run RFECV.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bff8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numX = df.select_dtypes(include=[\"number\"]).drop(columns=[\"churn\"], errors=\"ignore\")\n",
    "y = df[\"churn\"].astype(int) if \"churn\" in df.columns else (df.iloc[:, -1] > df.iloc[:, -1].median()).astype(int)\n",
    "mi = mutual_info_classif(numX.fillna(numX.median()), y, random_state=42)\n",
    "order = np.argsort(mi)[::-1]\n",
    "plt.bar(range(min(10, len(order))), mi[order][:10])\n",
    "plt.xticks(range(min(10, len(order))), numX.columns[order][:10], rotation=45, ha=\"right\")\n",
    "plt.title(\"Top MI Features\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "rfecv = RFECV(LogisticRegression(max_iter=300), cv=5, scoring=\"f1\")\n",
    "rfecv.fit(numX.fillna(numX.median()), y)\n",
    "print(\"Optimal features:\", rfecv.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc2432",
   "metadata": {},
   "source": [
    "## 12) Leakage-Safe Split & Pipelines\n",
    "\n",
    "**Why:** Prevent target leakage by fitting transforms only on training folds.  \n",
    "**Go-to:** `train_test_split`, `Pipeline`, `ColumnTransformer`, `cross_val_score`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db5306",
   "metadata": {},
   "source": [
    "### Storytime: Leakage-Safe Pipelines  \n",
    "Imagine training a detective using tomorrow’s newspaper. He solves cases perfectly—but cheats by knowing the future.  \n",
    "**Why do this step?** Pipelines ensure preprocessing fits only on training folds, not leaking test knowledge.  \n",
    "**Risk of skipping:** Models score brilliantly in practice but fail miserably in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e31e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "features = df.drop(columns=[c for c in [\"churn\"] if c in df.columns])\n",
    "target = df[\"churn\"].astype(int) if \"churn\" in df.columns else (df.iloc[:, -1] > df.iloc[:, -1].median()).astype(int)\n",
    "\n",
    "num_cols = features.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = features.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
    "\n",
    "num_pipe = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\")), (\"scale\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))])\n",
    "\n",
    "pre = ColumnTransformer([(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)])\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"select\", SelectKBest(mutual_info_classif, k=min(12, len(num_cols)+len(cat_cols)))), (\"clf\", LogisticRegression(max_iter=300))])\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(features, target, stratify=target, test_size=0.25, random_state=42)\n",
    "cv = cross_val_score(pipe, X_tr, y_tr, cv=5, scoring=\"f1\")\n",
    "print(\"CV F1:\", cv.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddefdec6",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "- Add `KBinsDiscretizer` into the numeric branch of the pipeline and evaluate.  \n",
    "- Swap `LogisticRegression` with `RandomForestClassifier` and compare F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26158fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# pipe2 = Pipeline([(\"pre\", pre), (\"clf\", RandomForestClassifier(random_state=42))])\n",
    "# scores = cross_val_score(pipe2, X_tr, y_tr, cv=5, scoring=\"f1\")\n",
    "# print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db7b1a",
   "metadata": {},
   "source": [
    "<details><summary><b>Answer Key — Step 12 (Pipelines)</b></summary>\n",
    "\n",
    "- Add KBins to numeric branch and compare RF baseline.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "features = df.drop(columns=[\"churn\"])\n",
    "target = df[\"churn\"].astype(int)\n",
    "\n",
    "num_cols = features.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = features.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"bins\", KBinsDiscretizer(n_bins=5, encode=\"ordinal\", strategy=\"quantile\")),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)])\n",
    "\n",
    "rf = Pipeline([(\"pre\", pre), (\"rf\", RandomForestClassifier(n_estimators=200, random_state=42))])\n",
    "scores = cross_val_score(rf, features, target, cv=5, scoring=\"f1\")\n",
    "print(\"RF with KBins CV F1:\", scores.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f174449",
   "metadata": {},
   "source": [
    "---\n",
    "# (Original walkthrough below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ef9f4",
   "metadata": {},
   "source": [
    "# Data Processing & Feature Engineering (3-Hour Hands‑On)\n",
    "\n",
    "**Audience:** Early professionals  \n",
    "**Focus:** Cleaning → Transformation → Feature Extraction  \n",
    "**Instructor Pack:** Each section has runnable code, notes, and mini‑exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd74598",
   "metadata": {},
   "source": [
    "## Agenda (3 Hours)\n",
    "\n",
    "- **0:00–0:10 (10 min)** — Kickoff & goals  \n",
    "- **0:10–0:35 (25 min)** — Data ingestion & quick EDA (tabular focus; touch text/datetime)  \n",
    "- **0:35–1:20 (45 min)** — Data cleaning (missing values, outliers, duplicates, schema)  \n",
    "- **1:20–2:05 (45 min)** — Transformations (scaling, encoding, binning, datetime, text basics)  \n",
    "- **2:05–2:40 (35 min)** — Feature extraction & selection (PCA, mutual info, filters)  \n",
    "- **2:40–3:00 (20 min)** — End‑to‑end Pipeline + leakage checks + Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e614a",
   "metadata": {},
   "source": [
    "## 0) Environment Setup\n",
    "\n",
    "> Tip: Run this once at the start of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif, VarianceThreshold, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Misc\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24568e",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Import NumPy/Pandas for data handling, Matplotlib for quick charts.\n",
    "- Bring in core scikit‑learn tools (imputers, encoders, scalers, pipelines, feature selectors, PCA, model, metrics).\n",
    "- Set Pandas display options so participants can see wide tables without truncation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9f1ec",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **SimpleImputer** replaces missing values with median (numeric) or most frequent (categorical).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e7151",
   "metadata": {},
   "source": [
    "## 1) Synthetic, Realistic Dataset\n",
    "\n",
    "We generate a small dataset with numeric, categorical, datetime, and a tiny text field—plus intentional issues: missing values, outliers, and duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198f29f",
   "metadata": {},
   "source": [
    "### Storytime: Schema & Dtype Validation  \n",
    "Imagine opening a mysterious chest of data treasures. Some jewels are labeled correctly (`int`, `float`, `datetime`), but others are mislabeled—numbers stored as strings or dates written like riddles.  \n",
    "**Why do this step?** Because if you don’t, downstream algorithms may stumble trying to add `\"2000\"` to `2000.0`.  \n",
    "**Risk of skipping:** Models may silently treat numbers as text, miss calculations, or break when handling nulls. Always check the labels on your jewels before trading them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb98ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(7)\n",
    "n = 500\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"age\": rng.integers(18, 70, size=n),\n",
    "    \"income\": rng.normal(60000, 15000, size=n).round(0),\n",
    "    \"tenure_years\": rng.exponential(3, size=n).round(2),\n",
    "    \"segment\": rng.choice([\"A\",\"B\",\"C\"], size=n, p=[0.5,0.3,0.2]),\n",
    "    \"region\": rng.choice([\"North\",\"South\",\"East\",\"West\"], size=n),\n",
    "    \"joined_on\": pd.to_datetime(\"2020-01-01\") + pd.to_timedelta(rng.integers(0, 1500, size=n), unit=\"D\"),\n",
    "    \"notes\": rng.choice([\"loves discounts\", \"calls often\", \"email only\", \"no preference\"], size=n),\n",
    "})\n",
    "\n",
    "# introduce some issues\n",
    "df.loc[rng.choice(n, 20, replace=False), \"income\"] = np.nan              # missing\n",
    "df.loc[rng.choice(n, 10, replace=False), \"age\"] = None                   # missing\n",
    "df.loc[rng.choice(n, 5, replace=False), \"tenure_years\"] = 99             # outliers\n",
    "dups = df.sample(5, random_state=1)\n",
    "df = pd.concat([df, dups], ignore_index=True)                            # duplicates\n",
    "\n",
    "# Binary target with a non-trivial relationship\n",
    "df[\"churn\"] = ((df[\"income\"].fillna(50000) < 55000) | (df[\"tenure_years\"] < 1.5) | (df[\"segment\"]==\"C\")).astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fafe9f",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Build a **synthetic but realistic** dataset with mixed types: numeric, categorical, datetime, and short text.\n",
    "- Intentionally inject **missing values**, **outliers**, and **duplicates** so we can practice cleaning.\n",
    "- Create a **target (`churn`)** with a non‑trivial relationship to features to validate feature engineering later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4dc5e",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- This cell applies transformations/analysis to progress through data cleaning or feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721e4530",
   "metadata": {},
   "source": [
    "## 2) Quick EDA (10–25 min)\n",
    "\n",
    "Check shape, schema, missingness, class balance, and distributions. Spot potential leakage and sanity‑check duplicates/outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52908739",
   "metadata": {},
   "source": [
    "### Storytime: Text Normalization  \n",
    "Think of your dataset as a classroom roll call. `\"North \"` and `\"north\"` are the same student arriving with different uniforms.  \n",
    "**Why do this step?** To ensure fairness—grouping all variants into one true identity.  \n",
    "**Risk of skipping:** Your model believes `\"North\"` and `\"north\"` are two separate regions, splitting loyalty and weakening insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0990c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape, df.dtypes)\n",
    "display(df.describe(include=\"all\").T.head(12))\n",
    "display(df.isna().mean().sort_values(ascending=False))\n",
    "display(df[\"segment\"].value_counts(normalize=True))\n",
    "ax = df[\"age\"].plot(kind=\"hist\", bins=20)\n",
    "ax.set_title(\"Age Distribution\")\n",
    "plt.show()\n",
    "display(pd.crosstab(df[\"segment\"], df[\"churn\"], normalize=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a02b4",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Quick **shape & schema** check to confirm row/column counts and dtypes.\n",
    "- `describe()` summarizes distributions; `isna().mean()` highlights **missingness** by column.\n",
    "- Histogram shows **distribution & skew**; crosstab inspects **class balance by segment**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb961f7",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **EDA functions** summarize schema, missingness, distributions, and target correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892c09f",
   "metadata": {},
   "source": [
    "## 3) Data Cleaning (35–80 min)\n",
    "\n",
    "We handle duplicates, imputations, outliers, and schema rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8118fbe",
   "metadata": {},
   "source": [
    "### Storytime: De-duplication  \n",
    "Imagine counting villagers in a town square. If you count the same person twice, your population size grows unfairly.  \n",
    "**Why do this step?** Duplicates inflate statistics, bias averages, and mislead predictions.  \n",
    "**Risk of skipping:** You may design resources (or models) based on phantom citizens, leading to wasted effort or wrong conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "after = df.shape[0]\n",
    "print(f\"Removed {before-after} duplicate rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16415d7e",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Remove exact duplicate rows to prevent **data leakage** and **biased metrics**.\n",
    "- The before/after print quantifies the clean‑up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5747d27d",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **drop_duplicates()** removes duplicate rows from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"age\", \"income\", \"tenure_years\"]\n",
    "cat_cols = [\"segment\", \"region\"]\n",
    "\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77363985",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Impute **numeric** features with median (robust to outliers) and **categoricals** with most‑frequent to keep valid categories.\n",
    "- Ensures downstream models receive **complete matrices** without NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b83300",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **SimpleImputer** replaces missing values with median (numeric) or most frequent (categorical).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d161981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap extreme tenure values at 95th percentile\n",
    "cap = df[\"tenure_years\"].quantile(0.95)\n",
    "df[\"tenure_years\"] = np.where(df[\"tenure_years\"] > cap, cap, df[\"tenure_years\"])\n",
    "\n",
    "# Unsupervised detection (inspect, then decide)\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.02)\n",
    "mask_inlier = lof.fit_predict(df[num_cols]) == 1\n",
    "print(\"Outliers flagged by LOF:\", (~mask_inlier).sum())\n",
    "# Optionally filter:\n",
    "# df = df[mask_inlier]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a43d9",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- **Cap** extreme values to reduce the influence of outliers (winsorization).\n",
    "- Use **LocalOutlierFactor** to *flag* unusual rows; teams can decide to drop or keep after inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87cdd6",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **LocalOutlierFactor** flags outliers based on neighborhood density differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic schema checks\n",
    "assert (df[\"age\"] >= 0).all(), \"Age must be non-negative\"\n",
    "assert df[\"segment\"].isin([\"A\",\"B\",\"C\"]).all()\n",
    "print(\"Schema checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf93e3",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Enforce **business rules** (age non‑negative, segment in {A,B,C}).\n",
    "- Early assertions catch schema drifts or bad upstream data before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea926c69",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- This cell applies transformations/analysis to progress through data cleaning or feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a478b1",
   "metadata": {},
   "source": [
    "## 4) Transformations (80–125 min)\n",
    "\n",
    "Scaling, encoding, binning, datetime expansion, and tiny text features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01501220",
   "metadata": {},
   "source": [
    "### Storytime: Missing Values  \n",
    "In an ancient scroll, some words have faded away. Do you ignore them, guess the missing letters, or fill them with placeholders?  \n",
    "**Why do this step?** Models can’t handle blanks—they need complete sentences.  \n",
    "**Risk of skipping:** Leaving gaps causes models to crash or hallucinate patterns, distorting results. The way you fill the blanks shapes the story told by the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b3840",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[num_cols] = scaler.fit_transform(df_scaled[num_cols])\n",
    "df_scaled[num_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8020798",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- **Standardize** numeric columns (zero mean, unit variance) so linear models and distance‑based methods behave well.\n",
    "- Store results in a copy (`df_scaled`) to keep a clean audit trail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3504c08",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **StandardScaler** scales numeric features to mean=0 and variance=1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot for nominal 'region'\n",
    "ohe = OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"ignore\")\n",
    "region_ohe = ohe.fit_transform(df_scaled[[\"region\"]])\n",
    "region_ohe = pd.DataFrame(region_ohe, columns=ohe.get_feature_names_out([\"region\"]))\n",
    "df_enc = pd.concat([df_scaled.drop(columns=[\"region\"]), region_ohe], axis=1)\n",
    "\n",
    "# Ordinal for ordered 'segment' (assume C < B < A)\n",
    "ord_enc = OrdinalEncoder(categories=[[\"C\",\"B\",\"A\"]])\n",
    "df_enc[\"segment_ord\"] = ord_enc.fit_transform(df[[\"segment\"]])\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3a810",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Apply **One‑Hot Encoding** to nominal `region` (drop one column to avoid dummy trap).\n",
    "- Apply **Ordinal Encoding** to `segment` where an order is assumed (C < B < A) for models that can use ordinality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6565b4e",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **OneHotEncoder** converts categorical variables into binary indicator columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = KBinsDiscretizer(n_bins=4, encode=\"ordinal\", strategy=\"quantile\")\n",
    "df_enc[\"income_bin\"] = kb.fit_transform(df[[\"income\"]]).astype(int)\n",
    "df_enc[\"age_bin\"] = kb.fit_transform(df[[\"age\"]]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129663c",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- **Discretize** continuous variables into bins to capture **non‑linear** effects and allow tree/linear models to exploit thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d698390",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **KBinsDiscretizer** bins continuous values into discrete intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.to_datetime(df[\"joined_on\"])\n",
    "df_enc[\"join_year\"]  = dt.dt.year\n",
    "df_enc[\"join_month\"] = dt.dt.month\n",
    "df_enc[\"tenure_days\"] = (pd.Timestamp.today().normalize() - dt).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc092ab",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Engineer **calendar** features (year, month) and **tenure in days** to turn datetime into useful numeric signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c1c88",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **Datetime accessor (.dt)** extracts parts like year, month, and computes durations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc[\"notes_len\"] = df[\"notes\"].str.len()\n",
    "for kw in [\"discount\", \"email\", \"call\"]:\n",
    "    df_enc[f\"kw_{kw}\"] = df[\"notes\"].str.contains(kw, case=False).astype(int)\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4450c63",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Create lightweight **text features** without full NLP: length and keyword presence flags (`discount`, `email`, `call`).\n",
    "- Great for tabular problems where text is short and domain‑specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679dbcca",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **str.contains** checks if keywords exist in text, creating binary features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9cb6a7",
   "metadata": {},
   "source": [
    "## 5) Feature Extraction & Selection (125–160 min)\n",
    "\n",
    "Variance filtering, Mutual Information, and PCA (unsupervised on numeric block)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc901fb",
   "metadata": {},
   "source": [
    "### Storytime: Outliers  \n",
    "Picture a feast where one guest eats 100 plates while everyone else eats 3. That single guest distorts the banquet’s average.  \n",
    "**Why do this step?** Outliers can dominate scales, shift means, and mislead models.  \n",
    "**Risk of skipping:** Your model may believe everyone is a glutton, overestimating costs or risks. Handling outliers keeps the feast realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = df_enc.drop(columns=[\"churn\", \"notes\", \"joined_on\", \"segment\"])\n",
    "y  = df[\"churn\"].astype(int)\n",
    "\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "X_vt = vt.fit_transform(X0)\n",
    "X_vt = pd.DataFrame(X_vt, columns=X0.columns[vt.get_support()])\n",
    "X_vt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60576b0",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Remove **near-constant** features that carry little to no information and can slow training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832bc065",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **VarianceThreshold** removes features with near-constant variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = mutual_info_classif(X_vt, y, random_state=42)\n",
    "mi_series = pd.Series(mi, index=X_vt.columns).sort_values(ascending=False)\n",
    "topk = 10\n",
    "display(mi_series.head(topk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec389491",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Use **Mutual Information** (non‑parametric) to score feature–target dependency and pick top‑k informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4f007",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **mutual_info_classif** scores features by dependency with the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=mutual_info_classif, k=topk)\n",
    "X_mi = selector.fit_transform(X_vt, y)\n",
    "selected_cols = X_vt.columns[selector.get_support()]\n",
    "selected_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e9118",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- **SelectKBest** keeps the k highest‑scoring features (here via MI), reducing dimensionality/noise before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce2260",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **mutual_info_classif** scores features by dependency with the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_only = [\"age\", \"income\", \"tenure_years\"]\n",
    "num_block = df_enc[num_cols_only].to_numpy()\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "num_pcs = pca.fit_transform(num_block)\n",
    "print(\"Explained variance (2 PCs):\", pca.explained_variance_ratio_.sum().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627c7bd",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Run **PCA** on numeric block to create low‑dimensional components capturing maximum variance; useful for compression and visualization.\n",
    "- We report explained variance to show how much signal the 2 PCs retain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c668a240",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **PCA** reduces dimensionality by projecting onto principal components while preserving variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe371b9d",
   "metadata": {},
   "source": [
    "## 6) End‑to‑End Pipeline (160–180 min)\n",
    "\n",
    "A leakage‑safe pipeline: imputation, scaling/encoding, selection, and model—**inside** CV folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d3150",
   "metadata": {},
   "source": [
    "### Storytime: Business Rules  \n",
    "Imagine a guard at the city gate checking travelers: no child under 18 may enter the tavern.  \n",
    "**Why do this step?** Business rules enforce domain reality—ages, ranges, memberships.  \n",
    "**Risk of skipping:** A 5-year-old might be counted as a loan applicant or a negative income might sneak into salary records, shattering trust and logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa92c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = df.drop(columns=[\"notes\", \"joined_on\"])  # drop raw text/datetime for this demo\n",
    "X = base.drop(columns=[\"churn\"])\n",
    "y = base[\"churn\"].astype(int)\n",
    "\n",
    "num_cols_pipe = X.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "cat_cols_pipe = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols_pipe),\n",
    "        (\"cat\", categorical_pipe, cat_cols_pipe),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_pipe = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"varth\", VarianceThreshold(0.0)),\n",
    "    (\"select\", SelectKBest(mutual_info_classif, k=12)),\n",
    "    (\"clf\", LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(model_pipe, X_train, y_train, cv=5, scoring=\"f1\")\n",
    "print(\"CV F1:\", cv_scores.round(3), \"mean:\", cv_scores.mean().round(3))\n",
    "\n",
    "model_pipe.fit(X_train, y_train)\n",
    "pred = model_pipe.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0962baa6",
   "metadata": {},
   "source": [
    "**What we achieve here**\n",
    "- Build a **leakage‑safe** end‑to‑end `Pipeline`:\n",
    "  - `ColumnTransformer` applies numeric/categorical preprocessing **inside CV folds**.\n",
    "  - `VarianceThreshold` and `SelectKBest` run **after** preprocessing, still inside CV.\n",
    "  - `LogisticRegression` is the baseline model.\n",
    "- Evaluate with **cross‑validated F1** and report a clean **classification report** on held‑out test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32027133",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This block demonstrates how the function(s) above work and what we achieve.\n",
    "- **SimpleImputer** replaces missing values with median (numeric) or most frequent (categorical).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895649b1",
   "metadata": {},
   "source": [
    "## Mini‑Exercises\n",
    "\n",
    "1. **Imputer swap:** Use `KNNImputer` for numerics; compare CV F1.  \n",
    "2. **Encoding change:** Use `OrdinalEncoder` for `segment` instead of One‑Hot; compare.  \n",
    "3. **Binning:** Add `KBinsDiscretizer` for `income` within the numeric pipeline.  \n",
    "4. **PCA in pipeline:** Insert a small PCA on numeric features before selection; observe impact.  \n",
    "5. **Outliers:** Filter 2% LOF outliers prior to training; discuss pros/cons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c57df1",
   "metadata": {},
   "source": [
    "## Common Pitfalls — Checklist\n",
    "\n",
    "- Leakage by fitting scalers/encoders/selectors **before** split/CV.  \n",
    "- Treating datetimes as raw numbers; extract parts or durations instead.  \n",
    "- High‑cardinality one‑hot blow‑ups (consider hashing or target encoding—use with care).  \n",
    "- Imbalanced targets → track precision/recall/F1, not just accuracy.  \n",
    "- Over‑engineered features that don’t generalize—validate with CV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4b8c6",
   "metadata": {},
   "source": [
    "## Optional Add‑Ons (Homework / Bonus)\n",
    "\n",
    "- **Text features:** Try `TfidfVectorizer` on `notes` via `ColumnTransformer`.  \n",
    "- **Leakage demo:** Add an obvious post‑event variable and see inflated CV.  \n",
    "- **Drift checks:** Compare train/test distributions (e.g., KS test, PSI)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
